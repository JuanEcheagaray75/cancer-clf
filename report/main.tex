\documentclass[journal]{IEEEtran}                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                         % paper
\IEEEoverridecommandlockouts        % This command is only
% needed if you want to
                                    % use the \thanks command
%\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document
% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{rotating} % rotate figures
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[spanish]{babel}
\usepackage{cite}

\usepackage{atbegshi} % erase first blank page
\usepackage{hyperref}

\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}}

\title{\LARGE \bf Proyecto: Minería de datos}

%%%%%%%%%%%%%%%%%%%%%% AUTHORS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\author{Juan Pablo Echeagaray González, Emily Rebeca Méndez Cruz, Grace Aviance Silva Arostegui}% <-this % stops 
\begin{document}

    \thanks{Juan Pablo Echeagaray González, Emily Rebeca Méndez Cruz, Grace Aviance Silva Arostegui pertencen al Tec de Monterrey campus Monterrey, N.L. C.P. 64849, Mexico {\tt\small}}

    \maketitle

    \thispagestyle{empty}
    \pagestyle{empty}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{abstract}
        Referencia perrona de este libro \cite{geron-2019}
    \end{abstract}

    \begin{IEEEkeywords} 
    Data Science, Machine Learning, Data Analysis
    \end{IEEEkeywords}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Introducción} \label{introduction}

    \section{Créditos} \label{credits}
       
        \begin{itemize}
            \item Juan Pablo Echeagaray González - Programador / Data Scientist
            \item Emily Rebeca Méndez Cruz - Programador / Data Scientist
            \item Grace Aviance Silva Aróstegui - Programador / Data Scientist
        \end{itemize}

    \section{Modelos de Machine Learning} \label{modelos}

        \subsection{Árbol de decisión: Emily Rebeca Méndez Cruz} \label{decision-tree}

            Los árboles de decisión son un método de aprendizaje supervisado utilizado para la clasificación y la regresión. Este método tiene como objetivo crear un modelo que prediga el valor de una variable de destino mediante el aprendizaje de reglas de decisión simples deducidas de las características de los datos proporcionados. Su estructura puede ser analizada para obtener más información acerca de la relación entre las características y el objeto a predecir.

            En esta representación un nodo interno representa un atributo o característica, la rama corresponde una regla de decisión y cada nodo u hoja representa el resultado. El nodo raíz es el nodo superior de un árbol de decisión.

            Al momento de crear un árbol, las observaciones de entrenamiento quedan agrupadas en los nodos terminales. Para predecir una nueva observación, se recorre el árbol en función del valor de sus predictores hasta llegar a uno de los nodos terminales.

            La metodología de un árbol de decisión es:
            \begin{itemize}
                \item Seleccionar el mejor atributo empleando una medida de selección de características o atributos
                \item Hacer del atributo seleccionado un nodo de decisión y dividir el conjunto de datos en subconjuntos más pequeños
                \item Comenzar la selección del árbol repitiendo el proceso en cada atributo hasta que una de estas condiciones se cumpla: \begin{itemize}
                    \item Todas las variables pertenecen al mismo valor de atributo
                    \item Se han acabado los atributos
                    \item No hay más casos
                \end{itemize}
            \end{itemize}

            Las reglas de partición ayudan a determinar puntos de ruptura para un conjunto en un nodo dado. La medida de selección de atributos es una heurística y esta proporciona un rango a cada característica, el atributo con la mejor puntuación se selecciona como atributo de división.

            Realizar el entrenamiento permite a nuestro modelo tener una base para su futura toma de decisiones, permitiendo obtener un aprendizaje inicial puesto que se le suministra información. Esto proporciona que el modelo pueda plantear correlaciones entre los datos del entrenamiento y los que tiene actualmente guiándolo a la opción correcta.
        
        \subsection{Support Vector Machine (SVM) Grace Aviance Silva Arostegui} \label{svm}
            
            Support vector machine  (SVM) es un algoritmo de aprendizaje supervisado que se utiliza en muchos problemas de clasificación y regresión, e incluso para la detección de valores atípicos \cite{geron-2019}. Este modelo de aprendizaje automático se basa en una separación de diferentes clases a través de un hiperplano en un espacio de dimensión superior \cite{deisenroth2020mathematics}.

            Dado un conjunto de muestras (ejemplos de entrenamiento) se etiquetan clases y se entrena una SVM para construir un modelo que prediga la clase de una nueva muestra.

            Para la base de datos que tenemos y el análisis que queremos llevar a cabo, en donde el enfoque es considerar en cada uno de los registros de las mujeres cual es el riesgo que tienen para desarrollar cáncer cervical. Dado que en la base de datos el rango de riesgos va de 0 a 4, tenemos así 5 clases de las cuales buscaríamos 5 hiperplanos que tengan el margen lo más ancho posible entre las clases, para así poder entregar lo mejor posible al algoritmo y hacer mejores clasificaciones futuras. Cabe mencionar que utilizamos un 80/20 de los datos para entrenamiento y prueba respectivamente.

            El objetivo de este algoritmo es encontrar un hiperplano que separe de la mejor forma posible las clases diferentes de puntos de datos, lo cual implica que el hiperplano tenga un margen lo más amplio posible entre todas las clases, paralela al hiperplano y que no tenga puntos de datos interiores o si acaso permite un número pequeño de clasificaciones erróneas.

        \subsection{Red Neuronal: Juan Pablo Echeagaray González} \label{neural-network}
            % uwu \cite{team-2022} \cite{team-2022}
            Después de inspeccionar el mapa generado hemos notado que hay algunos puntos que parecen tener datos geográficos erróneos, descartar la entrega a estos clientes es algo inaceptable, así que una de las siguientes tareas en el proyecto será desarrollar un método de limpieza efectivo que ayude a mejorar la información geográfica que obtengamos de cada punto.

        \subsection{Regresión Logística: Juan Pablo Echeagaray González} \label{logistic}

            La regresión logística es otro de los métodos de \emph{machine learning} usados comúnmente para la clasificación de datos. Este algoritmo se usa regularmente para estimar la posibilidad de que una instancia pertenezca a cierta clase; para el caso de clasificación binaria, si dicha probabilidad es mayor al 50\%, se clasifica a esa instancia como perteneciente a la clase positiva. Para generalizar este modelo a problemas de clasificación con $n$ clases $(n > 2)$, se calculan $n$ probabilidades de que la instancia pertenezca a la clase $i$, al final se escoge la que tenga el valor más alto \cite{geron-2019} \cite{sci-kit-learn-no-dateB}.

            Al igual que con una regresión lineal, este algoritmo calcula una suma ponderada de los atributos de la instancia, más un término libre; pero lo que el modelo regresa es el resultado de aplicar la función logística a ese número:

            \begin{equation}
                \hat{p} = \sigma(\bf{x}^{T}\bf{\Theta})
            \end{equation}

            La función logística tiene un rango de 0 a 1 (por eso su uso como clasificador binario) a su vez se define como:

            \begin{equation}
                \sigma(t) = \frac{1}{1 + \exp(-t)}
            \end{equation}

            Una vez que se cuenta con la estimación de la probabilidad, la salida del modelo queda definida como:

            \begin{equation}
                \hat{y} = \begin{cases}
                    0 & \hat{p} < 0.5 \\
                    1 & \hat{p} \geq 0.5
                \end{cases}
            \end{equation}

            Para entrenar nuestro modelo hemos de encontrar el vector $\bf{\Theta} \in \mathbb{R}^{27}$ (se tienen 27 atributos después de la limpieza de datos) que minimice la función \emph{log loss}, dicha función tiene la siguiente forma:
            
            \begin{equation} \label{log-loss}
                J(\bf{\Theta}) = - \frac{1}{m} \sum_{i=1}^{m} \left[y^{(i)} \log \left(\hat{p}^{(i)}\right) + (1 - y^{(i)}) \log \left(1 - \hat{p}^{(i)} \right) \right]
            \end{equation}

            El índice $m$ de la función \ref{log-loss} representa el número de instancias que tenemos en la base de datos; lo que hace esta función es calcular el promedio del error producido con el vector $\bf{\Theta}$. Al final de la fase de entrenamiento se desea haber encontrado el vector que minimice esta función

            \subsubsection{Generalizando a n clases}

                La regresión logística que clasifica instancias que podrían tener diferentes etiquetas es conocida como \emph{Regresión Logística Múltiple} o \emph{Regresión Softmax} (similar a la función descrita en la sección \ref{neural-network}). Primero se calcula una suma ponderada como en regresión logística para cada una de las posibles clases:

                \begin{equation}
                    s_k(\bf{x}) = \bf{x}^T \Theta^{(k)}
                \end{equation}

                Se usa este valor en la función sigmoide para obtener una probabilidad estimada:

                \begin{equation}
                    \hat{p}_k = \sigma(\bf{s}(\bf{x}))_k
                \end{equation}

                Y al final se escoge la clase con la probabilidad más alta:
                \begin{equation}
                    \hat{y} = \arg\max_k \sigma(s_k(\bf{x}))
                \end{equation}
            
            Para nuestro caso de estudio hemos optado por utilizar la librería \emph{scikit-learn} con su regresor logístico documentado en \cite{sci-kit-learn-no-dateB}. Los datos de entrenamiento del modelo representarán un 80\% de la base de datos, y la variable que buscaremos predecir será el nivel de riesgo que tiene un individuo dados los 27 atributos que tenemos. Una comparativa de este modelo contra los demás se realizará en la sección \ref{resultados}.

    \section{Resultados} \label{resultados}

    \section{Conclusiones} \label{conclusiones}
        
        \subsection{Áreas de mejora} \label{improvements}

        \subsection{Modelo seleccionado} \label{selected-model}

    \section{Reflexiones} \label{thoughts}
    
    \appendices
    
    \section{Datos}
        Los datos usados en este proyecto pueden descargarse \href{https://www.kaggle.com/code/ravaliraj/risk-classification-of-cervical-cancer}{aquí}

    \section{Código}
        El código desarrollado se encuentra en el siguiente \href{https://github.com/JuanEcheagaray75/cancer-clf}{repositorio}
    \section{Evidencias de trabajo en equipo}
    \bibliographystyle{IEEEtran}
    \bibliography{references.bib}

\end{document}