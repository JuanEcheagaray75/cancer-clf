\documentclass[journal]{IEEEtran}                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                         % paper
\IEEEoverridecommandlockouts        % This command is only
% needed if you want to
                                    % use the \thanks command
%\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document
% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{rotating} % rotate figures
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[spanish]{babel}
\usepackage{cite}

\usepackage{atbegshi} % erase first blank page
\usepackage{hyperref}

\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}}

\title{\LARGE \bf Proyecto: Minería de datos}

%%%%%%%%%%%%%%%%%%%%%% AUTHORS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\author{Juan Pablo Echeagaray González, Emily Rebeca Méndez Cruz, Grace Aviance Silva Arostegui}% <-this % stops 
\begin{document}

    \thanks{Juan Pablo Echeagaray González, Emily Rebeca Méndez Cruz, Grace Aviance Silva Arostegui pertencen al Tec de Monterrey campus Monterrey, N.L. C.P. 64849, Mexico {\tt\small}}

    \maketitle

    \thispagestyle{empty}
    \pagestyle{empty}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{abstract}
        Referencia perrona de este libro \cite{geron-2019}
    \end{abstract}

    \begin{IEEEkeywords} 
    Data Science, Machine Learning, Data Analysis
    \end{IEEEkeywords}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Introducción} \label{introduction}

    \section{Créditos} \label{credits}
       
        \begin{itemize}
            \item Juan Pablo Echeagaray González - A00830646
            \item Emily Rebeca Méndez Cruz
            \item Grace Aviance Silva Aróstegui
        \end{itemize}

    \section{Modelos de Machine Learning} \label{modelos}

        \subsection{Árbol de decisión} \label{decision-tree}
            \cite{sci-kit-learn-no-dateA}
        \subsection{Support Vector Machine (SVM) Grace Aviance Silva Arostegui} \label{svm}
            
            Support vector machine  (SVM) es un algoritmo de aprendizaje supervisado que se utiliza en muchos problemas de clasificación y regresión, e incluso para la detección de valores atípicos \cite{geron-2019}. Este modelo de aprendizaje automático se basa en una separación de diferentes clases a través de un hiperplano en un espacio de dimensión superior \cite{deisenroth2020mathematics}.

            Dado un conjunto de muestras (ejemplos de entrenamiento) se etiquetan clases y se entrena una SVM para construir un modelo que prediga la clase de una nueva muestra.

            Para la base de datos que tenemos y el análisis que queremos llevar a cabo, en donde el enfoque es considerar en cada uno de los registros de las mujeres cual es el riesgo que tienen para desarrollar cáncer cervical. Dado que en la base de datos el rango de riesgos va de 0 a 4, tenemos así 5 clases de las cuales buscaríamos 5 hiperplanos que tengan el margen lo más ancho posible entre las clases, para así poder entregar lo mejor posible al algoritmo y hacer mejores clasificaciones futuras. Cabe mencionar que utilizamos un 80/20 de los datos para entrenamiento y prueba respectivamente.

        \subsection{Red Neuronal} \label{neural-network}
            % uwu \cite{team-2022} \cite{team-2022}
            Después de inspeccionar el mapa generado hemos notado que hay algunos puntos que parecen tener datos geográficos erróneos, descartar la entrega a estos clientes es algo inaceptable, así que una de las siguientes tareas en el proyecto será desarrollar un método de limpieza efectivo que ayude a mejorar la información geográfica que obtengamos de cada punto.

        \subsection{Regresión Logística} \label{logistic}

            La regresión logística es otro de los métodos de \emph{machine learning} usados comúnmente para la clasificación de datos. Este algoritmo se usa regularmente para estimar la posibilidad de que una instancia pertenezca a cierta clase; para el caso de clasificación binaria, si dicha probabilidad es mayor al 50\%, se clasifica a esa instancia como perteneciente a la clase positiva. Para generalizar este modelo a problemas de clasificación con $n$ clases $(n > 2)$, se calculan $n$ probabilidades de que la instancia pertenezca a la clase $i$, al final se escoge la que tenga el valor más alto \cite{geron-2019} \cite{sci-kit-learn-no-dateB}.

            Al igual que con una regresión lineal, este algoritmo calcula una suma ponderada de los atributos de la instancia, más un término libre; pero lo que el modelo regresa es el resultado de aplicar la función logística a ese número:

            \begin{equation}
                \hat{p} = \sigma(\bf{x}^{T}\bf{\Theta})
            \end{equation}

            La función logística tiene un rango de 0 a 1 (por eso su uso como clasificador binario) a su vez se define como:

            \begin{equation}
                \sigma(t) = \frac{1}{1 + \exp(-t)}
            \end{equation}

            Una vez que se cuenta con la estimación de la probabilidad, la salida del modelo queda definida como:

            \begin{equation}
                \hat{y} = \begin{cases}
                    0 & \hat{p} < 0.5 \\
                    1 & \hat{p} \geq 0.5
                \end{cases}
            \end{equation}

            Para entrenar nuestro modelo hemos de encontrar el vector $\bf{\Theta} \in \mathbb{R}^{27}$ (se tienen 27 atributos después de la limpieza de datos) que minimice la función \emph{log loss}, dicha función tiene la siguiente forma:
            
            \begin{equation} \label{log-loss}
                J(\bf{\Theta}) = - \frac{1}{m} \sum_{i=1}^{m} \left[y^{(i)} \log \left(\hat{p}^{(i)}\right) + (1 - y^{(i)}) \log \left(1 - \hat{p}^{(i)} \right) \right]
            \end{equation}

            El índice $m$ de la función \ref{log-loss} representa el número de instancias que tenemos en la base de datos; lo que hace esta función es calcular el promedio del error producido con el vector $\bf{\Theta}$. Al final de la fase de entrenamiento se desea haber encontrado el vector que minimice esta función

            \subsubsection{Generalizando a n clases}

                La regresión logística que clasifica instancias que podrían tener diferentes etiquetas es conocida como \emph{Regresión Logística Múltiple} o \emph{Regresión Softmax} (similar a la función descrita en la sección \ref{neural-network}). Primero se calcula una suma ponderada como en regresión logística para cada una de las posibles clases:

                \begin{equation}
                    s_k(\bf{x}) = \bf{x}^T \Theta^{(k)}
                \end{equation}

                Se usa este valor en la función sigmoide para obtener una probabilidad estimada:

                \begin{equation}
                    \hat{p}_k = \sigma(\bf{s}(\bf{x}))_k
                \end{equation}

                Y al final se escoge la clase con la probabilidad más alta:
                \begin{equation}
                    \hat{y} = \arg\max_k \sigma(s_k(\bf{x}))
                \end{equation}

    \section{Resultados} \label{resultados}

    \section{Conclusiones} \label{conclusiones}
        
        \subsection{Áreas de mejora} \label{improvements}

        \subsection{Modelo seleccionado} \label{selected-model}

    \section{Reflexiones} \label{thoughts}
    
    \appendices
    
    \section{Datos}
        Los datos usados en este proyecto pueden descargarse \href{https://www.kaggle.com/code/ravaliraj/risk-classification-of-cervical-cancer}{aquí}

    \section{Código}
        El código desarrollado se encuentra en el siguiente \href{https://github.com/JuanEcheagaray75/cancer-clf}{repositorio}
    \section{Evidencias de trabajo en equipo}
    \bibliographystyle{IEEEtran}
    \bibliography{references.bib}

\end{document}